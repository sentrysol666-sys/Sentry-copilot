{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpIgdAVm/FYHd/qhTbjrtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sentrysol666-sys/Sentry-copilot/blob/main/llama3_1_8b_AML_CFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.4.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "PX13WM_wsLTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "clQxD3PzsQXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/dataset/solana-vuln-sim-10k.csv\"\n",
        "\n",
        "ext = os.path.splitext(dataset_path)[1].lower()\n",
        "rows = []\n",
        "\n",
        "if ext == \".jsonl\":\n",
        "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            combined_text = \" \".join([f\"{k}: {v}\" for k, v in data.items()])\n",
        "            rows.append({\"text\": combined_text})\n",
        "elif ext in [\".csv\", \".tsv\"]:\n",
        "    sep = \",\" if ext == \".csv\" else \"\\t\"\n",
        "    df = pd.read_csv(dataset_path, sep=sep)\n",
        "    for _, row in df.iterrows():\n",
        "        combined_text = \" \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
        "        rows.append({\"text\": combined_text})\n",
        "else:\n",
        "    raise ValueError(\"Unsupported format! Use .csv, .tsv, or .jsonl.\")\n",
        "\n",
        "dataset = Dataset.from_list(rows)\n",
        "print(\"Dataset loaded:\", dataset)"
      ],
      "metadata": {
        "id": "FbVzyXocsR16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/llama-3.2-3b-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name,\n",
        "    max_seq_length=2048,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(\"Base model loaded!\")"
      ],
      "metadata": {
        "id": "FC0aBQxXsvcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "fjUsNaWMvTuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=20,\n",
        "    max_steps=200,       # ubah sesuai dataset\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    output_dir=\"lora-llama32-3b\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    packing=True,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Starting training ...\")\n",
        "trainer.train()\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "QPQ7dNW8vWSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/drive/MyDrive/lora-llama32-3b\"\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"Fine-tuned model saved to {save_dir}\")\n"
      ],
      "metadata": {
        "id": "_ge3vHJIvZkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Reload base + attach adapter\n",
        "finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name,\n",
        "    max_seq_length=2048,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "finetuned_model.load_adapter(save_dir)\n",
        "\n",
        "# Pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=finetuned_model,\n",
        "    tokenizer=finetuned_tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "# Prompt contoh\n",
        "prompt = finetuned_tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"Jelaskan kerentanan reentrancy pada smart contract sederhana.\"}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "# Generate\n",
        "result = pipe(prompt)[0][\"generated_text\"]\n",
        "print(\"ðŸ”Ž Inference Result:\\n\", result)\n",
        "\n",
        "# Save output ke Google Drive\n",
        "with open(\"/content/drive/MyDrive/inference_result.txt\", \"w\") as f:\n",
        "    f.write(result)\n",
        "\n",
        "print(\"Inference result saved to Google Drive!\")"
      ],
      "metadata": {
        "id": "CmFmFZmivtKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}